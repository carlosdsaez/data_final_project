{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow\n",
    "import keras\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data_wrangled.csv', index_col = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>floor</th>\n",
       "      <th>price</th>\n",
       "      <th>price_m2</th>\n",
       "      <th>rooms</th>\n",
       "      <th>size</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>226000.0</td>\n",
       "      <td>2897.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>40.430409</td>\n",
       "      <td>-3.557889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>98500.0</td>\n",
       "      <td>1790.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>40.423733</td>\n",
       "      <td>-3.561187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.420332</td>\n",
       "      <td>129000.0</td>\n",
       "      <td>1842.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>40.430736</td>\n",
       "      <td>-3.635022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.420332</td>\n",
       "      <td>220000.0</td>\n",
       "      <td>3098.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>40.384267</td>\n",
       "      <td>-3.663003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>148000.0</td>\n",
       "      <td>2144.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>40.343037</td>\n",
       "      <td>-3.708971</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bathrooms     floor     price  price_m2  rooms  size   latitude  longitude\n",
       "0        1.0  4.000000  226000.0    2897.0    2.0  78.0  40.430409  -3.557889\n",
       "1        1.0  0.000000   98500.0    1790.0    2.0  55.0  40.423733  -3.561187\n",
       "2        1.0  2.420332  129000.0    1842.0    3.0  70.0  40.430736  -3.635022\n",
       "3        1.0  2.420332  220000.0    3098.0    3.0  71.0  40.384267  -3.663003\n",
       "4        1.0  0.000000  148000.0    2144.0    2.0  69.0  40.343037  -3.708971"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_neural = df.drop(['floor_was_missing', 'price_cut', 'index', 'price_m2'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = df_neural.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.00000000e+00,  4.00000000e+00,  2.26000000e+05, ...,\n",
       "         7.80000000e+01,  4.04304094e+01, -3.55788870e+00],\n",
       "       [ 1.00000000e+00,  0.00000000e+00,  9.85000000e+04, ...,\n",
       "         5.50000000e+01,  4.04237329e+01, -3.56118680e+00],\n",
       "       [ 1.00000000e+00,  2.42033195e+00,  1.29000000e+05, ...,\n",
       "         7.00000000e+01,  4.04307359e+01, -3.63502170e+00],\n",
       "       ...,\n",
       "       [ 2.00000000e+00,  2.42033195e+00,  5.58000000e+05, ...,\n",
       "         1.58000000e+02,  4.04981190e+01, -3.86182970e+00],\n",
       "       [ 2.00000000e+00,  2.00000000e+00,  2.17329000e+05, ...,\n",
       "         1.19000000e+02,  4.04598286e+01, -3.46135260e+00],\n",
       "       [ 2.00000000e+00,  0.00000000e+00,  4.71000000e+05, ...,\n",
       "         1.45000000e+02,  4.04146709e+01, -3.88483280e+00]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate((dataset[:,:2], dataset[:,3:]), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = dataset[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "226000.0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.       ,  4.       ,  2.       , 78.       , 40.4304094,\n",
       "       -3.5578887])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X_scale = min_max_scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val_and_test, Y_train, Y_val_and_test = train_test_split(X_scale, Y, test_size=0.3)\n",
    "X_val, X_test, Y_val, Y_test = train_test_split(X_val_and_test, Y_val_and_test, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(10, activation='relu', input_shape=(6,)),\n",
    "    Dense(10, activation='relu'),\n",
    "    Dense(1, activation='sigmoid'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='sgd',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8313 samples, validate on 1782 samples\n",
      "Epoch 1/100\n",
      "8313/8313 [==============================] - 0s 54us/step - loss: -5722757.1231 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 2/100\n",
      "8313/8313 [==============================] - 0s 54us/step - loss: -5722757.1201 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 3/100\n",
      "8313/8313 [==============================] - 0s 54us/step - loss: -5722757.0906 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 4/100\n",
      "8313/8313 [==============================] - 0s 49us/step - loss: -5722757.1016 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 5/100\n",
      "8313/8313 [==============================] - 1s 60us/step - loss: -5722757.0847 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 6/100\n",
      "8313/8313 [==============================] - 1s 62us/step - loss: -5722757.1127 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 7/100\n",
      "8313/8313 [==============================] - 1s 70us/step - loss: -5722757.1012 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 8/100\n",
      "8313/8313 [==============================] - 0s 51us/step - loss: -5722757.1049 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 9/100\n",
      "8313/8313 [==============================] - 0s 53us/step - loss: -5722757.0811 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 10/100\n",
      "8313/8313 [==============================] - 0s 55us/step - loss: -5722757.1134 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 11/100\n",
      "8313/8313 [==============================] - 0s 54us/step - loss: -5722757.1015 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 12/100\n",
      "8313/8313 [==============================] - 0s 51us/step - loss: -5722757.1192 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 13/100\n",
      "8313/8313 [==============================] - 0s 58us/step - loss: -5722757.1051 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 14/100\n",
      "8313/8313 [==============================] - 1s 60us/step - loss: -5722757.1348 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 15/100\n",
      "8313/8313 [==============================] - 0s 55us/step - loss: -5722757.0783 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 16/100\n",
      "8313/8313 [==============================] - 0s 52us/step - loss: -5722757.0595 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 17/100\n",
      "8313/8313 [==============================] - 0s 51us/step - loss: -5722757.0938 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 18/100\n",
      "8313/8313 [==============================] - 0s 53us/step - loss: -5722757.1142 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 19/100\n",
      "8313/8313 [==============================] - 0s 52us/step - loss: -5722757.1095 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 20/100\n",
      "8313/8313 [==============================] - 1s 63us/step - loss: -5722757.1065 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 21/100\n",
      "8313/8313 [==============================] - 0s 53us/step - loss: -5722757.0871 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 22/100\n",
      "8313/8313 [==============================] - 0s 51us/step - loss: -5722757.0880 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 23/100\n",
      "8313/8313 [==============================] - 1s 65us/step - loss: -5722757.1095 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 24/100\n",
      "8313/8313 [==============================] - 1s 64us/step - loss: -5722757.0882 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 25/100\n",
      "8313/8313 [==============================] - 1s 65us/step - loss: -5722757.0761 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 26/100\n",
      "8313/8313 [==============================] - 1s 66us/step - loss: -5722757.0866 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 27/100\n",
      "8313/8313 [==============================] - 0s 52us/step - loss: -5722757.0950 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 28/100\n",
      "8313/8313 [==============================] - 1s 70us/step - loss: -5722757.1051 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 29/100\n",
      "8313/8313 [==============================] - 0s 59us/step - loss: -5722757.0912 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 30/100\n",
      "8313/8313 [==============================] - 0s 58us/step - loss: -5722757.0751 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 31/100\n",
      "8313/8313 [==============================] - 0s 57us/step - loss: -5722757.1000 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 32/100\n",
      "8313/8313 [==============================] - 1s 68us/step - loss: -5722757.0642 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 33/100\n",
      "8313/8313 [==============================] - 0s 56us/step - loss: -5722757.1252 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 34/100\n",
      "8313/8313 [==============================] - 1s 60us/step - loss: -5722757.1274 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 35/100\n",
      "8313/8313 [==============================] - 1s 61us/step - loss: -5722757.0957 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 36/100\n",
      "8313/8313 [==============================] - 0s 58us/step - loss: -5722757.0727 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 37/100\n",
      "8313/8313 [==============================] - 0s 52us/step - loss: -5722757.0952 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 38/100\n",
      "8313/8313 [==============================] - 0s 56us/step - loss: -5722757.0733 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 39/100\n",
      "8313/8313 [==============================] - 0s 60us/step - loss: -5722757.0926 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 40/100\n",
      "8313/8313 [==============================] - 1s 71us/step - loss: -5722757.0925 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 41/100\n",
      "8313/8313 [==============================] - 1s 66us/step - loss: -5722757.1030 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 42/100\n",
      "8313/8313 [==============================] - 0s 57us/step - loss: -5722757.0998 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 43/100\n",
      "8313/8313 [==============================] - 0s 55us/step - loss: -5722757.1070 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 44/100\n",
      "8313/8313 [==============================] - 1s 63us/step - loss: -5722757.0989 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 45/100\n",
      "8313/8313 [==============================] - 1s 62us/step - loss: -5722757.1070 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 46/100\n",
      "8313/8313 [==============================] - 1s 64us/step - loss: -5722757.1009 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 47/100\n",
      "8313/8313 [==============================] - 0s 55us/step - loss: -5722757.1021 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 48/100\n",
      "8313/8313 [==============================] - 0s 57us/step - loss: -5722757.0718 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 49/100\n",
      "8313/8313 [==============================] - 1s 77us/step - loss: -5722757.0841 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 50/100\n",
      "8313/8313 [==============================] - 1s 65us/step - loss: -5722757.0966 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 51/100\n",
      "8313/8313 [==============================] - 0s 55us/step - loss: -5722757.0520 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 52/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8313/8313 [==============================] - 0s 51us/step - loss: -5722757.0932 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 53/100\n",
      "8313/8313 [==============================] - 0s 58us/step - loss: -5722757.0642 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 54/100\n",
      "8313/8313 [==============================] - 0s 56us/step - loss: -5722757.1179 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 55/100\n",
      "8313/8313 [==============================] - 0s 49us/step - loss: -5722757.0919 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 56/100\n",
      "8313/8313 [==============================] - 1s 73us/step - loss: -5722757.0996 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 57/100\n",
      "8313/8313 [==============================] - 1s 63us/step - loss: -5722757.0574 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 58/100\n",
      "8313/8313 [==============================] - 0s 60us/step - loss: -5722757.1186 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 59/100\n",
      "8313/8313 [==============================] - 0s 50us/step - loss: -5722757.1087 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 60/100\n",
      "8313/8313 [==============================] - 0s 58us/step - loss: -5722757.1216 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 61/100\n",
      "8313/8313 [==============================] - 0s 55us/step - loss: -5722757.1193 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 62/100\n",
      "8313/8313 [==============================] - 1s 91us/step - loss: -5722757.1095 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 63/100\n",
      "8313/8313 [==============================] - 1s 139us/step - loss: -5722757.1084 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 64/100\n",
      "8313/8313 [==============================] - 1s 103us/step - loss: -5722757.0980 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 65/100\n",
      "8313/8313 [==============================] - 1s 112us/step - loss: -5722757.1222 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 66/100\n",
      "8313/8313 [==============================] - 0s 56us/step - loss: -5722757.1054 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 67/100\n",
      "8313/8313 [==============================] - 1s 70us/step - loss: -5722757.0942 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 68/100\n",
      "8313/8313 [==============================] - 1s 74us/step - loss: -5722757.0982 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 69/100\n",
      "8313/8313 [==============================] - 1s 63us/step - loss: -5722757.0877 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 70/100\n",
      "8313/8313 [==============================] - 0s 51us/step - loss: -5722757.1002 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 71/100\n",
      "8313/8313 [==============================] - 0s 52us/step - loss: -5722757.0899 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 72/100\n",
      "8313/8313 [==============================] - 0s 58us/step - loss: -5722757.0923 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 73/100\n",
      "8313/8313 [==============================] - 1s 64us/step - loss: -5722757.0992 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 74/100\n",
      "8313/8313 [==============================] - 0s 51us/step - loss: -5722757.0792 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 75/100\n",
      "8313/8313 [==============================] - 0s 59us/step - loss: -5722757.0987 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 76/100\n",
      "8313/8313 [==============================] - 1s 64us/step - loss: -5722757.1037 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 77/100\n",
      "8313/8313 [==============================] - 1s 65us/step - loss: -5722757.1019 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 78/100\n",
      "8313/8313 [==============================] - 1s 62us/step - loss: -5722757.1120 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 79/100\n",
      "8313/8313 [==============================] - 1s 88us/step - loss: -5722757.0935 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 80/100\n",
      "8313/8313 [==============================] - 1s 77us/step - loss: -5722757.0970 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 81/100\n",
      "8313/8313 [==============================] - 1s 62us/step - loss: -5722757.1251 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 82/100\n",
      "8313/8313 [==============================] - 1s 77us/step - loss: -5722757.1208 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 83/100\n",
      "8313/8313 [==============================] - 0s 57us/step - loss: -5722757.1085 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 84/100\n",
      "8313/8313 [==============================] - 0s 59us/step - loss: -5722757.0944 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 85/100\n",
      "8313/8313 [==============================] - 0s 52us/step - loss: -5722757.0873 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 86/100\n",
      "8313/8313 [==============================] - 0s 57us/step - loss: -5722757.0713 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 87/100\n",
      "8313/8313 [==============================] - 0s 51us/step - loss: -5722757.1143 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 88/100\n",
      "8313/8313 [==============================] - 0s 59us/step - loss: -5722757.1216 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 89/100\n",
      "8313/8313 [==============================] - 0s 51us/step - loss: -5722757.0970 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 90/100\n",
      "8313/8313 [==============================] - 0s 57us/step - loss: -5722757.0796 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 91/100\n",
      "8313/8313 [==============================] - 0s 52us/step - loss: -5722757.1025 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 92/100\n",
      "8313/8313 [==============================] - 0s 53us/step - loss: -5722757.1063 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 93/100\n",
      "8313/8313 [==============================] - ETA: 0s - loss: -5702608.6687 - acc: 0.0000e+ - 0s 55us/step - loss: -5722757.1052 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 94/100\n",
      "8313/8313 [==============================] - 0s 48us/step - loss: -5722757.1243 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 95/100\n",
      "8313/8313 [==============================] - 0s 50us/step - loss: -5722757.1096 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 96/100\n",
      "8313/8313 [==============================] - 0s 49us/step - loss: -5722757.1280 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 97/100\n",
      "8313/8313 [==============================] - 0s 54us/step - loss: -5722757.1112 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 98/100\n",
      "8313/8313 [==============================] - 0s 48us/step - loss: -5722757.0971 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 99/100\n",
      "8313/8313 [==============================] - 0s 48us/step - loss: -5722757.0731 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n",
      "Epoch 100/100\n",
      "8313/8313 [==============================] - 0s 54us/step - loss: -5722757.1105 - acc: 0.0000e+00 - val_loss: -5511284.1510 - val_acc: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(X_train, Y_train,\n",
    "          batch_size=32, epochs=100,\n",
    "          validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1782/1782 [==============================] - 0s 26us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, Y_test)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([173000., 164544., 700000., ..., 480000., 208000., 275000.])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
