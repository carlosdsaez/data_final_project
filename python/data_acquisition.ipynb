{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from itertools import cycle\n",
    "from requests.exceptions import ProxyError\n",
    "from lxml.html import fromstring\n",
    "import ast\n",
    "with open('../.env', 'r') as f:\n",
    "    key = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions to get proxis\n",
    "\n",
    "#Proxies obtained from the file proxies.txt. where the proxies are pasted from https://www.proxy-list.download/es/HTTPS\n",
    "#Before launching the process, it is recommended to paste the latest proxies\n",
    "def get_proxies():\n",
    "    proxie_file = 'proxies.txt'\n",
    "    try:\n",
    "        with open(proxie_file) as f:\n",
    "            proxies = f.readlines()\n",
    "        return set(proxies)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "#Proxies obtained by scraping https://free-proxy-list.net/\n",
    "def get_proxies2():\n",
    "    url = 'https://free-proxy-list.net/'\n",
    "    headers = {}\n",
    "    agent = ua_random()\n",
    "    headers['user-agent'] = agent\n",
    "    response = requests.get(url, headers = headers)\n",
    "    parser = fromstring(response.text)\n",
    "    proxies = set()\n",
    "    for i in parser.xpath('//tbody/tr')[:10]:\n",
    "        if i.xpath('.//td[7][contains(text(),\"yes\")]'):\n",
    "            #Grabbing IP and corresponding PORT\n",
    "            proxy = \":\".join([i.xpath('.//td[1]/text()')[0], i.xpath('.//td[2]/text()')[0]])\n",
    "            proxies.add(proxy)\n",
    "    return proxies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to get a random agent for each request, from the file agents.txt\n",
    "def ua_random():\n",
    "    random_ua = ''\n",
    "    ua_file = 'agents.txt'\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        with open(ua_file) as f:\n",
    "            lines = f.readlines()\n",
    "        if len(lines) > 0:\n",
    "            random_ua = random.choice(lines)\n",
    "            random_ua = random_ua.split('\\n')\n",
    "            \n",
    "    except Exception as ex:\n",
    "        print('Exception in ua_random')\n",
    "        print(str(ex))\n",
    "        \n",
    "    finally:\n",
    "        return random_ua[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to get the content of a given URL as parameter\n",
    "\n",
    "def scrape_url(url, headers):\n",
    "    proxies = get_proxies()\n",
    "    proxy_pool = cycle(proxies)\n",
    "    flag = False\n",
    "    \n",
    "    while flag == False:\n",
    "        proxy = next(proxy_pool)\n",
    "        try:\n",
    "            r = requests.get(url, headers = headers, proxies={\"http\": proxy})\n",
    "            r.raise_for_status()\n",
    "            flag = True\n",
    "            return BeautifulSoup(r.content)\n",
    "        except requests.exceptions.HTTPError as e_http:\n",
    "            pass\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            pass\n",
    "        \n",
    "    if flag == False:\n",
    "        # none of the proxies worked\n",
    "        raise ProxyError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to parse each web page obtained in every request\n",
    "\n",
    "def parser_page(content):\n",
    "    anuncios = content.find_all('div', 'information')\n",
    "    pisos = []\n",
    "    \n",
    "    for piso in anuncios:\n",
    "        items = {}\n",
    "        items['location1'] = piso.find('a', 'anuncioLink').string.replace('Piso en ', '')\n",
    "        items['location2'] = str(piso.find('div', 'location').string).strip()\n",
    "        items['price'] = piso.find('div', 'price').contents[0].strip().split(' ')[0]\n",
    "        items['description'] = str(piso.find('div', 'description').string).strip()\n",
    "        chars = piso.find('div', 'characteristics').find_all('div', 'item')\n",
    "        \n",
    "        for char in chars:\n",
    "            if '€/m²' in char.contents[0].strip():\n",
    "                items['price_m2'] = char.contents[0].strip().split(' ')[0].strip()\n",
    "            elif 'm²' in char.contents[0]:\n",
    "                items['size'] = char.contents[0].strip().split(' ')[0]\n",
    "            elif char.find('span', 'icoBed') != None:\n",
    "                items['rooms'] = int(char.contents[0].strip().split(' ')[0])\n",
    "                #print(items)\n",
    "            elif char.find('span', 'icoBath') != None:\n",
    "                items['bathrooms'] = int(char.contents[0].strip().split(' ')[0])\n",
    "            elif 'planta' in char.contents[0]:\n",
    "                items['floor'] = char.contents[0].strip().split(' ')[0]\n",
    "            elif 'Bajo' in char.contents[0]:\n",
    "                items['floor'] = 0 \n",
    "                \n",
    "        pisos.append(items)\n",
    "    return pisos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to launch the process, for a given URL, a given range of pages and header values \n",
    "#adapted to the web that is going to be scraped.\n",
    "\n",
    "def kickstart(URL_general, range_to_scrape, headers):\n",
    "    datos_pisos = []\n",
    "    \n",
    "    for i in range_to_scrape:\n",
    "        agent = ua_random()\n",
    "        headers['user-agent'] = agent\n",
    "        content = scrape_url(URL_general % i, headers)\n",
    "        datos_pisos += parser_page(content)\n",
    "        time.sleep(random.randint(2, 5))\n",
    "        \n",
    "    return datos_pisos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to obtain the coordinates from Google Geocode API from the address\n",
    "def get_position(x):\n",
    "    url = 'https://maps.googleapis.com/maps/api/geocode/json?address='\n",
    "    try:\n",
    "        r = requests.get(url + '+'.join(x.split(' ')) + '&key=' + key)\n",
    "        datos = r.json()\n",
    "        location = datos['results'][0]['geometry']['location']\n",
    "        #time.sleep(2)\n",
    "        return location\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to generate the column with latitude and longitude for the dataframe provided\n",
    "def get_value_from_dict(dict, x):\n",
    "    try:\n",
    "        return dict[x]\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def create_positions_field(df):\n",
    "    df['location'] = df['location1'] + ', ' + df['location2']\n",
    "    dict_pos = {}\n",
    "    \n",
    "    for location in df['location']:\n",
    "        dict_pos[location] = get_position(location)\n",
    "    df['position'] = df.loc[:, 'location'].apply(lambda x: get_value_from_dict(dict_pos, x))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final execution of the process to obtain the raw dataset\n",
    "\n",
    "URL_general = 'https://www.pisos.com/venta/piso-madrid/%s/'\n",
    "range_to_scrape = range(1, 520)\n",
    "headers = {'Accept-Encoding': 'gzip, deflate, br',\n",
    "          'Accept-Language': 'en-US,en;q=0.9,es-ES;q=0.8,es;q=0.7',\n",
    "          'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3',\n",
    "          'Cache-Control': 'max-age=0',\n",
    "          'Upgrade-Insecure-Requests': '1',\n",
    "          'Refereer': 'https://www.pisos.com/venta/piso-madrid/',\n",
    "          'Host': 'www.pisos.com'}\n",
    "\n",
    "items = kickstart(URL_general, range_to_scrape, headers)\n",
    "\n",
    "data_set_raw = pd.DataFrame(items)\n",
    "data_set_raw_w_loc = create_positions_field(data_set_raw)\n",
    "\n",
    "#Dataframe exported as CSV\n",
    "data_set_raw_w_loc.to_csv('house_data_total.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
